{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import traceback \n",
    "import hashlib\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "\n",
    "import rx\n",
    "import rx.operators as ops\n",
    "import diart.operators as dops\n",
    "from diart import OnlineSpeakerDiarization, PipelineConfig \n",
    "from diart.sources import WebSocketAudioSource\n",
    "\n",
    "from source import clair\n",
    "from source import speech\n",
    "from source.utils import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "HOST = os.environ.get(\"HOST\")\n",
    "PORT = int(os.environ.get(\"PORT\"))\n",
    "WHISPER_SIZE = os.environ.get(\"WHISPER_SIZE\")\n",
    "WHISPER_COMPRESS_RATIO_THRESHOLD = float(os.environ.get(\"WHISPER_COMPRESS_RATIO_THRESHOLD\"))\n",
    "WHISPER_NO_SPEECH_THRESHOLD = float(os.environ.get(\"WHISPER_NO_SPEECH_THRESHOLD\"))\n",
    "PIPELINE_MAX_SPEAKERS = int(os.environ.get(\"PIPELINE_MAX_SPEAKERS\"))\n",
    "PIPELINE_DURATION = float(os.environ.get(\"PIPELINE_DURATION\"))\n",
    "PIPELINE_STEP = float(os.environ.get(\"PIPELINE_STEP\"))\n",
    "PIPELINE_SAMPLE_RATE = int(os.environ.get(\"PIPELINE_SAMPLE_RATE\"))\n",
    "PIPELINE_TAU = float(os.environ.get(\"PIPELINE_TAU\"))\n",
    "PIPELINE_RHO = float(os.environ.get(\"PIPELINE_RHO\"))\n",
    "PIPELINE_DELTA = float(os.environ.get(\"PIPELINE_DELTA\"))\n",
    "PIPELINE_CHUNK_DURATION = float(os.environ.get(\"PIPELINE_CHUNK_DURATION\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLAIR_URL = os.environ.get(\"CLAIR_URL\")\n",
    "\n",
    "req = clair.activate_configuration(\n",
    "    mode='ssrl', \n",
    "    language='EN', \n",
    "    keywords=['force', 'energy conservation', 'kinectic', 'potential'],\n",
    "    host=CLAIR_URL\n",
    ")\n",
    "req.status_code, req.reason, req.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline params. haven't tinkered with them much. you can also set device=torch.device(\"cuda\")\n",
    "speech_config = PipelineConfig(\n",
    "    duration=PIPELINE_DURATION,\n",
    "    step=PIPELINE_STEP, # When lower is more accurate but slower\n",
    "    latency=\"min\",  # When higher is more accurate but slower\n",
    "    tau_active=PIPELINE_TAU, # suggested by diart paper \n",
    "    rho_update=PIPELINE_RHO, # suggested by diart paper\n",
    "    delta_new=PIPELINE_DELTA,  # suggested by diart paper\n",
    "    device=\"cuda\",\n",
    "    max_speakers=PIPELINE_MAX_SPEAKERS,\n",
    ")\n",
    "pprint(speech_config.__dict__, indent=2)\n",
    "\n",
    "# Split the stream into chunks of seconds for transcription\n",
    "transcription_duration = 10 # seconds\n",
    "# Apply models in batches for better efficiency\n",
    "batch_size = int(transcription_duration // speech_config.step)\n",
    "\n",
    "# Suppress whisper-timestamped warnings for a clean output\n",
    "logging.getLogger(\"whisper_timestamped\").setLevel(logging.ERROR)\n",
    "# Set the whisper model size, you can also set device=\"cuda\"\n",
    "asr = speech.WhisperTranscriber(model=WHISPER_SIZE, device=\"cuda\")\n",
    "dia = OnlineSpeakerDiarization(speech_config)\n",
    "\n",
    "# Instantiate a new dialogue\n",
    "dialogue = []\n",
    "group_id = hashlib.md5(pd.Timestamp.now().strftime('%Y%m%d%H%M%S').encode()).hexdigest()[:6]\n",
    "last_processed_turn = {'last_turn': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up audio sources\n",
    "# source = MicrophoneAudioSource(config.sample_rate)\n",
    "source = WebSocketAudioSource(speech_config.sample_rate, HOST, PORT)\n",
    "\n",
    "# Chain of operations to test message helper for the stream of microphone audio\n",
    "source.stream.pipe(\n",
    "    # Format audio stream to sliding windows of 5s with a step of 500ms\n",
    "    dops.rearrange_audio_stream(\n",
    "        speech_config.duration, speech_config.step, speech_config.sample_rate\n",
    "    ),\n",
    "    # Wait until a batch is full. The output is a list of audio chunks\n",
    "    ops.buffer_with_count(count=batch_size),\n",
    "    # Obtain diarization prediction. The output is a list of pairs `(diarization, audio chunk)`\n",
    "    ops.map(dia),\n",
    "    # Concatenate 500ms predictions/chunks to form a single 2s chunk\n",
    "    ops.map(speech.concat),\n",
    "    # Modify ASR to handle empty chunks\n",
    "    ops.map(lambda ann_wav: ('', '') if ann_wav[0].get_timeline().duration() == 0 else asr(*ann_wav)),\n",
    "    # Modify transcription step to handle empty inputs\n",
    "    ops.map(lambda speaker_caption: '' if speaker_caption == ('', '') else speech.message_transcription(speaker_caption)),\n",
    "    # Buffering transcriptions until there is a new turn, adjusted to handle multiple turns\n",
    "    ops.map(lambda text: clair.buffering_turn(text, dialogue, group_id, \n",
    "                                              turn_threshold=5,\n",
    "                                              silence_threshold=3, \n",
    "                                              last_processed_turn=last_processed_turn,\n",
    "                                              verbose=True)),\n",
    "    # Use flat_map to handle each turn in the list individually\n",
    "    ops.flat_map(lambda turns: rx.from_iterable(turns)),\n",
    "    # Filter out empty turns based on the 'text' content\n",
    "    ops.filter(lambda turn: 'text' not in turn or turn['text'].strip() != ''),\n",
    "    # Send to API and get response\n",
    "    ops.map(lambda turn: clair.send_to_api_and_get_response(**turn, dialogue=dialogue, host=CLAIR_URL, verbose=True))\n",
    ").subscribe(\n",
    "    on_next=lambda output: (print(output), source.send(output)) if output else None,\n",
    "    on_error=lambda _: traceback.print_exc()  # print stacktrace if error\n",
    ")\n",
    "\n",
    "# Save the original stdout\n",
    "original_stdout = sys.stdout\n",
    "try:\n",
    "    # Initialize the logger\n",
    "    sys.stdout = Logger(f'logs/{group_id}.txt', original_stdout)\n",
    "    print(f\"Listening... {group_id}\")\n",
    "    # Stream of data\n",
    "    source.read()  \n",
    "finally:\n",
    "    print(\"Stopped listening\")\n",
    "    # Ensure the log file is properly closed\n",
    "    sys.stdout.log.close()\n",
    "    # Restore stdout to its original state\n",
    "    sys.stdout = original_stdout\n",
    "\n",
    "    print(dialogue)\n",
    "    # Export transcribed dialogue to a csv file\n",
    "    for msg in dialogue:\n",
    "        msg['group'] = group_id\n",
    "        msg['text'] = f'\"{msg[\"text\"]}\"' if '\"' not in msg['text'] else f\"{msg['text']}\"\n",
    "    pd.DataFrame(dialogue)[['group', 'username', 'timestamp', 'text']]\\\n",
    "        .to_csv(f'logs/{group_id}.csv', index=False, sep=\"|\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f2f",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
