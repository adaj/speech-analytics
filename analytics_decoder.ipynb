{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytics-decoder\n",
    "\n",
    "Note: This code serves as a non-functional boilerplate for developing other methods for decoding desirable analytics from speech interfaces using the code implemented in the `source/` folder. Again, this notebook does not yet work a mocked usecase example, but it is a starting point for developing a more functional version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import traceback \n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv\n",
    "from diart import OnlineSpeakerDiarization, PipelineConfig \n",
    "from diart.sources import WebSocketAudioSource\n",
    "import diart.operators as dops\n",
    "import rx.operators as ops\n",
    "\n",
    "from source import analytics\n",
    "from source import speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "HOST = os.environ.get(\"HOST\")\n",
    "PORT = int(os.environ.get(\"PORT\"))\n",
    "WHISPER_SIZE = os.environ.get(\"WHISPER_SIZE\")\n",
    "WHISPER_COMPRESS_RATIO_THRESHOLD = float(os.environ.get(\"WHISPER_COMPRESS_RATIO_THRESHOLD\"))\n",
    "WHISPER_NO_SPEECH_THRESHOLD = float(os.environ.get(\"WHISPER_NO_SPEECH_THRESHOLD\"))\n",
    "PIPELINE_MAX_SPEAKERS = int(os.environ.get(\"PIPELINE_MAX_SPEAKERS\"))\n",
    "PIPELINE_DURATION = float(os.environ.get(\"PIPELINE_DURATION\"))\n",
    "PIPELINE_STEP = float(os.environ.get(\"PIPELINE_STEP\"))\n",
    "PIPELINE_SAMPLE_RATE = int(os.environ.get(\"PIPELINE_SAMPLE_RATE\"))\n",
    "PIPELINE_TAU = float(os.environ.get(\"PIPELINE_TAU\"))\n",
    "PIPELINE_RHO = float(os.environ.get(\"PIPELINE_RHO\"))\n",
    "PIPELINE_DELTA = float(os.environ.get(\"PIPELINE_DELTA\"))\n",
    "PIPELINE_CHUNK_DURATION = float(os.environ.get(\"PIPELINE_CHUNK_DURATION\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline params. haven't tinkered with them much. you can also set device=torch.device(\"cuda\")\n",
    "speech_config = PipelineConfig(\n",
    "    duration=5,\n",
    "    step=0.5, # When lower is more accurate but slower\n",
    "    latency=\"min\",  # When higher is more accurate but slower\n",
    "    tau_active=0.555, # suggested by diart paper \n",
    "    rho_update=0.422, # suggested by diart paper\n",
    "    delta_new=1.517,  # suggested by diart paper\n",
    "    device=\"cuda\",\n",
    "    max_speakers=2,\n",
    ")\n",
    "pprint(speech_config.__dict__, indent=2)\n",
    "\n",
    "# Split the stream into chunks of seconds for transcription\n",
    "transcription_duration = 10 # seconds\n",
    "# Apply models in batches for better efficiency\n",
    "batch_size = int(transcription_duration // speech_config.step)\n",
    "\n",
    "# Suppress whisper-timestamped warnings for a clean output\n",
    "logging.getLogger(\"whisper_timestamped\").setLevel(logging.ERROR)\n",
    "# Set the whisper model size, you can also set device=\"cuda\"\n",
    "asr = speech.WhisperTranscriber(model=WHISPER_SIZE, device=\"cuda\")\n",
    "dia = OnlineSpeakerDiarization(speech_config)\n",
    "\n",
    "# Set up audio sources\n",
    "# source = MicrophoneAudioSource(config.sample_rate)\n",
    "source = WebSocketAudioSource(speech_config.sample_rate, \"localhost\", 5000)\n",
    "\n",
    "# Instantiate a new dialogue\n",
    "dialogue_state = analytics.DialogueState()\n",
    "\n",
    "# Chain of operations to test message helper for the stream of microphone audio\n",
    "source.stream.pipe(\n",
    "    # Format audio stream to sliding windows of 5s with a step of 500ms\n",
    "    dops.rearrange_audio_stream(\n",
    "        speech_config.duration, speech_config.step, speech_config.sample_rate\n",
    "    ),\n",
    "    # Wait until a batch is full. The output is a list of audio chunks\n",
    "    ops.buffer_with_count(count=batch_size),\n",
    "    # Obtain diarization prediction. The output is a list of pairs `(diarization, audio chunk)`\n",
    "    ops.map(dia),\n",
    "    # Concatenate 500ms predictions/chunks to form a single 2s chunk\n",
    "    ops.map(speech.concat),\n",
    "    # Ignore this chunk if it does not contain speech\n",
    "    ops.filter(lambda ann_wav: ann_wav[0].get_timeline().duration() > 0),\n",
    "    # Obtain speaker-aware transcriptions. The output is a list of pairs `(speaker: int, caption: str)`\n",
    "    ops.starmap(asr),\n",
    "    # Transcriptions\n",
    "    ops.map(speech.message_transcription),\n",
    "    # Buffering transcriptions until there is a new turn\n",
    "    ops.map(lambda text: analytics.buffering_turn(text, dialogue_state.turns_list, verbose=True)),\n",
    "    # Filter out None inputs\n",
    "    ops.filter(lambda turn: turn is not None), \n",
    "    # Send to API and get response\n",
    "    ops.map(lambda turn: analytics.compute_turn(**turn, dialogue_state=dialogue_state, verbose=True))\n",
    ").subscribe(\n",
    "    on_next=print,\n",
    "    on_error=lambda _: traceback.print_exc()  # print stacktrace if error\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running this cell, start the client_mic.py script to listen to a remote audio stream\n",
    "print(\"Listening...\")\n",
    "source.read()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
